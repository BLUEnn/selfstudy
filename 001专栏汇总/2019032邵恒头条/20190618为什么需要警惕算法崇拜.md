# 017 | 为什么需要警惕「算法崇拜」？
> 邵恒头条
2019-06-18

前两天，我在朋友圈看到很多人在转发一条关于人工智能换脸的新闻。

这条新闻说的是，现在用人工智能技术生成人脸，已经能做到以假乱真的地步。比如说，你可以在一个视频里，把杨幂的脸换到《射雕英雄传》里面朱茵的脸上，丝毫看不出违和感。你还可以生成假的社交账号，创造一个看起来非常像真人，但实则不存在的人的头像。这种现象，叫做深伪。

这件事听了让人有点毛骨悚然，因为它让我们意识到，在一个算法驱动的世界里，有很多信息都是真假难辨的，你不知道什么是可以信任的。

不过其实，即使没有深伪这种现象存在，我们对算法的信任可能也早已经超出了很多人的想象。

今天的《邵恒头条》，我想来跟你分享，前两天我在《华尔街日报》财富板块里看到一篇文章。这篇文章的标题，叫做《为什么投资人应该警惕自动化的投资建议》。

文章的作者是纽约城市大学商学院的一位教授。这位教授做了一个有意思的实验：她邀请了 800 个测试者，告诉这些人他们要依据顾问的建议，来做某些投资决策。有一拨测试者被告知，这些建议是真人投资顾问提供给他们的，而另一拨测试者被告知，这些建议一自动化的算法提供给他们的。然后教授让测试者评估一下，他们对于顾问给的投资建议，有多大信心。

结果实验发现，人们对于算法顾问给的建议，比对于真人顾问给的建议，更有信心。也就是说，人们认为算法的判断比人的判断更准确。

做完这一轮测试之后，这位教授又跟测试者说：很不幸，这些投资建议的结果最后都不怎样，都没赚钱。现在，你们要不要重新评估一下这些投资顾问？你还会接着用他们吗？结果发现，测试者对于算法顾问的信心仍然比对真人顾问的信心要高。

这个结果，乍一听你可能会觉得，这不意外啊。因为我们都知道，投资是一个高度复杂的、需要理性分析的事。人会受到情绪和主观意愿的影响，但是算法是客观的，是理性的。所以算法做出的决策，很多人天然会觉得更公正、更精准。

但是，这种认知恰恰是一个误区。算法也是人设计的。在设计的时候，会把我们人自己的偏见注入到算法里面。

你可能在得到很多老师的文章里，都看到过算法偏见的问题：比如那些用来预测个人犯罪行为的模型，会对少数族裔有偏见，因为用来训练算法的数据本身，就是有文化偏见的。

再比如说，有人曾经做过实验，如果你在谷歌上搜索一个黑人常用的人名，这之后谷歌就会有更高概率，给你推送刑满释放人员需要的服务广告，这其实就是种族歧视渗入算法的例子。

不过这篇文章的价值，还不在于它指出这个误区。这篇文章的价值在于，它通过这个实验提醒我们，已经有很多人在不知不觉当中进入了一种算法崇拜，把算法当成一种权威来看待。

哈佛大学在 2018 年对这个现象，做过一个专门的研究。在研究里，他们让 1200 多名参与测试的人，对未来进行预测。有的是要预测某些商业事件发生的概率，有的是要预测百强单曲的排名，有的要预测政治事件，还有的要做在线媒人。

在测试者做了预测之后，实验设计了一个特殊环节：他们告诉被测试人，我这有一个其他人关于预测的建议，你要不要参考？你可以考虑参考建议，修改你的预测。有的人收到的建议号称是来自于真人；有的人收到的建议，则是来自于算法。结果发现，同一条建议，如果实验者说是来自于算法而不是来自于人，那么这条建议会更有可能被采纳。而且这种情况出现在所有的预测领域里，不管预测的是商业世界，还是百强单曲，抑或是在线媒人。

从这个实验里我们可以看到，算法崇拜不仅是在投资这个领域存在，它其实是一个在逐渐泛化的现象。这种现象，我们在生活里其实早已经习以为常了。出门打车的时候，很多人不都是更相信高德地图生成的路线，而不相信司机师傅的判断吗？

那你可能会好奇，这样的趋势会带来什么样的后果呢？

《华尔街日报》的这篇文章指出，算法崇拜的一个后果，是人们可能会丢失看问题的多元视角。

打个比方来说吧，平时你要是做投资决策，可能会听好几个投资顾问的决定，从中选出一个最符合你心意的，或者综合这些人的意见做一个决策。但是如果给你投资建议的是一个权威，像巴菲特一样的人，你觉得他肯定不会错，那么你还会去寻求别人的建议吗？可能就不会了。

如果我们把算法看成是一种比人更客观、更可靠的权威，那么就有可能出现这样的情况。

除此之外，人们也可能变得更不爱冒险：如果有算法这样的权威在时时提供正确答案，那么还有什么冒险的必要呢？

这样的困局怎么破呢？毕竟，算法的准确性会持续提升，似乎算法成为权威这件事是大势所趋。

说到这，我就想来跟你说说下一个材料了。这个材料是我前段时间关注的一场，发生在斯坦福大学的人工智能论战。

这场论战的主角，是《人类简史》的作者尤瓦尔·赫拉利和人工智能专家李飞飞。在论战里，有一个他们激烈争论的问题，是人工智能是否会代替人类，成为决策的主体。

当时，赫拉利提出一个假想。他说，按照人工智能现在发展的趋势，总有一天，人工智能会比你自己还了解自己。到那个时候，人是不是就要把所有的决策权，都让渡出来呢？你在哪工作、能学什么专业，甚至是跟谁约会结婚——这样的决定，由最了解你的那一方来做，是不是最好呢？

而且，我们也不需要算法完美地了解我们自己才去做决定。我们只需要算法，比我们自己了解自己多一点点，就能做这件事了。这么一来，算法在未来社会，不就成了掌控决策权的老大哥么？而且，谁掌控了这样的算法，谁就有了控制整个社会的工具。赫拉利认为，这是一个危险的趋势。

这其实不是一个新问题，很多哲学家也提出过类似的问题。但是我推荐这场论战的原因，是李飞飞的回答。你可能知道，李飞飞是人工智能领域炙手可热的计算机科学家，在斯坦福任教的同时，还给谷歌的人工智能发展做顾问。

李飞飞说，当我们考虑这些哲学层面的问题时候，好像整个世界唯一存在的就是两个群体：一个是强大无比的人工智能，另一个就是创造了这些强大人工智能的一小撮人。

但是真实的社会远比这个要复杂，这里面有跨国的合作，有法律政策，有道德条款等等，总之，除了算法以外，还有很多玩家和规则。

如果不是因为这些玩家和规则的存在，那过去不是有很多的技术发明都能对人类造成毁灭性的破坏么？枪支、原子弹、生物科技等等。就连我们最基础的技术发明，火，那不也是既能生火做饭，也能引发火灾么？

所以任何一种技术都是一把双刃剑，就看它在什么条件下被使用。这就需要社会里的各个玩家和规则发挥作用了。不能把算法的运行空间想象成一个真空，能毫不受限地向各个领域延展触角。

这个讨论，其实对算法崇拜这个问题非常有启发。算法崇拜的解决方案其实不在算法本身，而在于规则的制定和共识的形成。《华尔街日报》的文章提供了以下的解决方案：

第一，无论是人工智能领域的创业者，还是法律的制定者，或者是媒体，都应该不断地提醒人们：即使用算法做决策，也要寻求第三方视角，不管这个视角是来自于人，还是来自于其他算法。

第二，如果涉及到人们的重大利益，比如说像是投资回报、健康这样的问题，那么算法的发明者也应该主动公布，算法里面是否可能有偏见，或者是公布算法的一些基本假设。这就好比说，一家药品公司要在说明书上公布，药品可能会有哪些副作用一样。规则的制定者，要主动引导人们用批判性的视角，来看待算法。

好，总结一下。在今天的《邵恒头条》中，我跟你分享了一个正处在萌芽中的现象，算法崇拜。

其实现在，依靠算法、人工智能做决策，已经变成家常便饭了，对有些行业来说甚至成了必需品，如果你的产品没有算法，反而显得不够专业，不够前沿。

但是，今天的材料恰恰是提醒我们，有一个维度是我们不能摒弃的，就是人这个维度。你是相信算法猜你喜欢，还是相信一个你信赖的朋友或者导师，给你推荐一篇他们喜欢的文章？你是相信一个算法自动生成投资决策，还是相信一个行业里摸爬滚打多年的投资人给你的专业建议呢？

人本身有偏好和局限，算法也是一样。一套真正有效的决策机制，一定是在机器和人的两个维度之间，找到了平衡和互补。