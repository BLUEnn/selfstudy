# 030 | 算法会让你的视野越来越窄吗？
> 邵恒头条
2019-07-05

今天我跟脱不花讨论了一个关于算法推荐的问题：假设你要给一个点餐 APP 设计推荐机制，那么以下这个场景你会怎么处理——你发现用户连着吃了两顿日料，那么接下来，APP 上是应该继续推荐日料呢，还是应该推荐川菜？

这个问题，在算法工程师当中是很有争议的。算法要实现的目的很单一，就是要让你完成点餐这个动作。但人的决策过程是多维的，点餐这么一个简单的动作，里面也有无数个维度要考虑，比如你是不是喜欢吃日料、会不会已经吃腻了、你可能是在什么场景下吃，等等等等。算法要想影响人的行为，并没有我们想象得那么容易。 

最近，我看到一篇有意思的文章，就讨论了一个与此相关的话题：推荐算法会不会让人只选择看自己喜欢的东西。我们经常听到的说法是，猜你喜欢这样的算法会让你只选择自己熟悉、喜欢的内容。时间久了，我们的视野就会越来越窄。

这种现象，被称为「 信息回音室 」。也就是说，你只听到跟自己观点相似的声音，这就像是你自己声音的回音。

但是这篇文章的作者，把国外学术机构对这个问题的主要研究都看了一遍，告诉我们结论是，并没有出现信息回音室的现象。其中一个原因，便是影响人选择的因素非常多元，而这些因素在不断地突破算法的边界。

今天的《邵恒头条》，我想来跟你分享一下这篇文章的观点。文章的作者是方可成博士，他曾经担任《南方周末》的记者，后来到美国宾夕法尼亚大学的传播学院做博士研究，今年刚刚获得学位。

首先，作者提醒我们，算法的类型很多元，并不是所有的算法，都会造成信息同质化的趋势。

比如说，现在有一种常用的算法，是「协同过滤算法」。这种算法的基本思路，就是找到和你喜好相同的人，把他们的选择也推荐给你。也就是说，这种推荐并不是完全基于你之前看了什么内容，把相似的内容推荐给你，而是也要基于和你类似的人在看什么。

明尼苏达大学计算机系的几位研究者，就曾经分析这样的算法对于信息同质化的影响。他们分析了一个电影网站 Movie Lens 的用户行为，这个网站就用到了协同过滤算法。

他们把用户分成了两组。其中一组用户不相信「猜你喜欢」推荐的内容，一组呢，相信「猜你喜欢」的推荐，并且愿意根据推荐来购买喜欢的电影。

他们研究了将近 2 年的数据后发现，每个人都找到了自己的偏好，选择电影的广度的确有所下降。但是相比而言，使用「猜你喜欢」的人，比不使用的人，购买的电影反而更多样。

所以，首先我们要知道的一点是，我们一直担心的算法技术，从结果上来看，并不一定会造成视野变窄，信息同质化。有些算法，反而可能会拓宽人的视野。

不过我们都知道，现在有一些社交网络，为了流量和商业利益，会只给你推荐，你想看的内容。比如说，脸书 Facebook 的广告算法。它会根据你过去的浏览和点赞记录，对你看到的朋友圈进行排列。那些更容易让你点赞、留言的朋友圈，会排列在上面，让你最先看到。这样的算法，听起来很容易造成信息同质化。

可是研究结果却不是这样的。在 2018 年，两位丹麦的学者选取了 1000 名丹麦的脸书用户，分析了他们 2 周之内在脸书上分享的朋友圈。他们的假设是，如果脸书的算法造成了人们接触到的信息或者圈子变窄，那么这些人分享的内容，应该也有很高的趋同性。

但结果发现，只有不到 10% 的人分享的链接是雷同的，不到 30% 的用户发布的内容文本是有相似性的。这个比例比研究人员的预期要低很多。

为什么呢？研究人员分析，这是因为脸书的算法虽然能决定朋友圈排列的顺序，但有一个因素是算法无法影响的，那就是我们跟谁做朋友，我们有多少朋友。这些因素，都是算法无法掌控的。

我们的社交关系也在影响着我们摄取信息的广度。算法可以在我们的社交圈里挑挑拣拣，但是没法左右我们真实生活中的社交圈。

好，接下来我们说说最后一点，个人偏好对信息同质化的影响。

我们总认为算法会造成信息同质化，这件事有一个前提，就是我们觉得人们只爱看自己熟悉的内容。

但是，文章作者所看到的心理学、传播学研究的结果，却并不支持这一点。这些研究显示，我们人不仅喜欢自己熟悉的东西、认同的东西，我们还喜欢超出我们预料的东西。

神经科学的研究显示，我们成年人的大脑很喜欢意料之外的东西。首先，意外会激活我们大脑里的杏仁核，让感官对于外界刺激的处理更敏锐。此外，意外的出现还会刺激海马体分泌多巴胺，我们都知道，多巴胺是让人产生快感的。因此从生理上来说，我们就乐于探索意料之外的东西。这种偏好，会驱使我们主动突破信息的边界，而这也是当前的算法无法囊括的。

说到这，你可能也会好奇，既然这么多研究的结果，都显示推荐算法并没有造成信息同质化，那为什么有这么多人持有这种看法呢？算法的负面效应，是怎么被夸大的呢？

方博士的这篇文章里提供了一个视角，我觉得非常有意思。他引用奈特基金会的一个研究解释说，之所以会出现夸大的现象，是因为人们为了跟自己的价值观保持一致而扭曲了真相。

什么意思呢？奈特基金会做了一个政治倾向的调研。他们发现，美国保守派的选民，会说自己平时主要看偏保守的福克斯新闻。但如果你去看他们的网页浏览记录，你会发现，其实这些保守派也会看自由派的新闻，比如 CNN 和《纽约时报》。那为什么他们要隐瞒这个信息呢？研究者认为，这是因为人们会把看什么新闻，跟自己的政治理念、价值观等同。我看什么新闻，就意味着我是什么人嘛。当然，这件事不一定是人们在刻意撒谎，人们也有可能是选择性地记忆。

同样的道理，人们可能嘴上抱怨，我被社交媒体上的算法限制住了，但实际上他们浏览的内容，可能远比他们认为的要丰富。

总结一下，今天的《邵恒头条》跟你分享了推荐算法是否会造成信息同质化，让人们的视野变窄。方可成博士梳理了最近几年发表在学术期刊上的研究。大部分研究的答案都是否定的。

算法多种多样，现在很常见的协同过滤算法不仅不会让视野变窄，反而会让我们接触的信息多元化。不过，即使有些算法的确会带来负面效应，但我们每个人手里也都有抵御这种效应的武器，那就是广泛的社交关系，以及我们与生俱来的好奇心。这些元素都是算法没有办法掌控的。

方可成博士的[文章链接](https://media.weibo.cn/article?id=2309404389564880915107)。

此外，我在第0期《邵恒头条》里，也介绍过两种突破算法局限的尝试，如果你感兴趣可以再去听听那期节目，为什么有人要过完全随机的生活？