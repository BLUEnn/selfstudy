# 0404. 新电车难题：用算法来分配社会悲剧
> 30天认知训练营·2018
2018-02-21

王烁亲述：

今天我们要讨论的是一个两难问题，面对社会困境，有没有一个社会能够长期接受的选择。

先从一个经典的伦理学思想实验，电车难题开始讲起。我把这个问题稍稍做了一些转化，把它放在了人工智能时代。

## 01. 机器人三定律

人工智能时代已经开始，如何与人工智能共处，或者说在我们还有能力的时候，给人工智能定什么规矩，不再是抽象的哲学问题，也不必上升到谁做主宰这类终级追问。它变得极为具体，比如下面这个场景：

行人横穿马路，刹车来不及，如果不转向，会撞死行人；如果转向，乘客会死于翻车。自动驾驶汽车应该作何选择？

这个问题已经迫在眉睫。自动驾驶汽车是最接近大规模商用的人工智能应用。无论中国，还是美国，多家公司已经上路实测，不止一家宣布要在一两年内推出自动出租车。

有汽车就有事故，有事故就有死伤，由人工智能来断谁该死谁该无恙，它该怎么断？

很早以前，人工智能、机器人刚刚出现在人类想象之中，人们就已想到要给它们定规。科幻小说大师阿西莫夫提出影响力极大的机器人三定律。

第一定律：机器人不得伤害人类，或者因不作为而使人类受到伤害；

第二定律：除非违背第一定律，否则机器人必须服从人类的命令；

第三定律：在不违背第一及第二定律下，机器人必须保护自己。

机器人三定律定义严密，层层递进，它能解决自动驾驶汽车的选择困境吗？

不能。

看第一定律：机器人不得伤害人类，或者因不作为而使人类受到伤害。自动汽车不转向就撞行人，转向则乘客死伤，都会伤害人类，它应该作为还是不作为，还有，哪个算作为哪个算不作为？我觉得机器人想这些问题能想到死机。

面对类似挑战，阿西莫夫后来给机器人三定律打了个补丁，在最前面加上第零定律：机器人不得伤害人类种族，或者因不作为而使人类种族受到伤害。

可以把第零定律理解为要机器人的选择与人类的最大整体利益相符。问题是怎么辨别最大整体利益是什么？像金庸小说《笑傲江湖》中神医平一指那样救一人杀一人，算不算？救一个小孩杀一个老人呢？救两人杀一人呢？救两个胖子杀一个瘦子呢？救两个女人杀一个男人呢？

无穷无尽的计算，根本没有正解。机器人还不如死机算了。

## 02. 你想要什么样的道德算法

自动驾驶汽车眼前就要上路了，机器人三定律不够用，怎样给它立个什么规矩？换句话说，你想机器人按什么道德算法来运行？

首先，你得了解自己到底想要的是什么。 我给你推荐个自测工具，道德机器（Moral Machine），它放在[麻省理工大学网站](http://moralmachine.mit.edu)上。点进去，你会遇到 13 种情境，形形色色的人群组合，老的小的男的女的好的坏的胖的瘦的。面对着自动汽车，假设你是乘客，你希望自动汽车牺牲谁拯救谁？

我的推荐我先测，于是知道了我自己的偏好：

孩子重于老人，胖瘦男女对我完全没差别；

多个人重于一个人，不论是什么人；

人重于动物，遇到撞人还是撞狗，永远选择撞狗；

如果转向不转向都撞到同样多人，那就不转向；

如果转向的后果是我自己完蛋，那就绝不转向。

推荐你们也去做一遍，一分钟做完，对自己了解得很多。

道德机器不是做着玩的，研究者用它来搜集在自动驾驶问题上社会的道德判断。此前已经在顶级期刊《科学》（Science)发表论文，标题就叫「自动驾驶的社会困境」（The Social Dilemma of Autonomous Vehicles）。

读过论文，我发现自己的选择很有代表性。总的来说，我的选择是 功利主义(utilitarianism) 的。如果一个选择比其他选择更有效用，我就选它。

一般来说，对社会而言，救多个人比救一个人更有效用，救人比救动物更有效用。研究者发现，效用主义深入人心，绝大多数人支持用效用主义来给自动汽车编制算法。

更有意思的是，研究里说了，人工智能采用功利主义算法来做决策，本身并不会让人们感到特别不舒服。就是说，在没有自动汽车的时候，我们在开车面临这些情境时的选择是功利主义的，如果这些选择将来由人工智能替我们做了，这件事本身不会那么令人难接受。

我们知道人生有许多悲剧，必须有取舍，谁来做都得取舍。取舍就有错，人能够接受机器犯错。问题在于，人是自相矛盾的。

以我为例，我认可效用主义算法，但如果自动汽车按这个算法来作选择，我却不想坐更不会买。救多个人优于救一个人，哪怕这个人是乘客，这样做决定的自动汽车，你敢坐吗？你想买吗？

研究者发现，绝大多数人不敢，不想。他们希望买的是那种永远优先保护乘客的自动汽车。也就是说， 绝大多数人都支持自动汽车使用功利主义算法，支持别人买这样的车，但自己不买。这就会造成典型的社会困境。你希望别人做的事，自己不做。结果就是谁也不做，最后这种自动汽车根本就没人买。

功利主义不行，并不是说换个算法就行。假如换个算法，永远优先保护乘客，你倒是愿意买了，但公众能允许这样赤裸裸地以行人为壑的做法吗？ 

与效用主义针锋相对的另一种道德算法是 康德式道德律令 ，它认为人是目的不是手段，一个人等于全人类，那更是让人工智能无从抉择。

说到这里，对道德哲学有了解的朋友，你应该已经对我开头说的，自动汽车撞谁不撞谁的算法问题，就是古老的电车难题在今天的重现，有了更深的理解。

电车难题是这样的： 电车失控，转向要伤人，不转向也要伤人，如果你是司机，该作何选择？百年来各种道德思想流派竞相抢答，没有一个公认的正解。今天无非是我们把司机换成了人工智能。

这道选择题人类给不出正解，人工智能自然也给不出。

## 03. 摆脱「新电车难题」的困境

难道自动汽车就上不了路吗？

这倒绝不会。

第一个摆脱困境的思路来自人工智能专家。 他们认为既然解决不了这个问题，那就消灭它。

谷歌的自动驾驶工程师说，道德算法是假问题。自动汽车能高速处理速度、距离、路况、天气等信息，用激光雷达和各种传感器提前感知，提前计算出最合理方案，使那些难以抉择的危险情境根本就没有机会发生。

问它撞一个人还是撞两个人，这个问题它回答不了，但是这问题在它那里不存在。工程师对技术魔法有谜之自信，不管你信不信，反正我是不信。

第二个思路是寻找与困境并存的策略。人类古往今来一直在做这件事。

电车难题、人工智能的道德算法，本质上都是把悲剧分配给谁的问题。在理论上不存在满足各种公平正义要求的正解，但实践中则随时随地都在分配，一刻也没有因为不够公平而停止过，问题只在于它是如何分配的。

这正是卡拉布雷西（Guido Calabresi）名著《悲剧性选择》（Tragic Choices: The conflicts society confronts in the allocation of tragically scarce resources） 最有洞察力的地方。

卡拉布雷西与科斯、波斯纳并列法经济学三位创始人之一，做耶鲁法学院院长多年，美国法律界的泰斗级人物。这本书的副题是「社会在分配悲剧性稀缺资源时面临的冲突」，专讲社会怎么分配悲剧： 怎么确定悲剧总量，以什么方式分配给谁。

泰坦尼克号撞冰山，谁先上救生船？计划生育，如何分配生育指标？器官移植，谁优先获得器官？以及今天的新问题：自动汽车撞谁不撞谁？等等等等？

回答这些问题只能是全社会的责任。在《悲剧性选择》中，卡拉布雷斯说， 社会分配悲剧有四个策略：市场、政府、摇号、惯例，但没有哪个能长期维持分配的稳定性。

1. 市场化分配是分散决策，价高者得。

	但是这种机制，不可避免地将已有的财富不平等，延展到当下分配的不平等，并且注定将人们认为不可定价的东西，比如生命，也贴上价签。
	没有一个社会能允许用市场化分配一切。

2. 政治分配的好处是较能反映民意，如果政治力量的对比产生自选票的话；但这也使得混乱成为现代社会常态：
	如果政府直接出面分配悲剧，那就成为社会价值观冲突的替罪羊；民意如潮汐，总在分配谁去承受悲剧的政府，无法长期稳定地获得民意支持。

3. 抽签把一切交给运气，看上去是绝对平等，但抹杀了被社会珍视的另外一些平等观：为什么不先救孩子？为什么不把机会让给那些有巨大贡献的人？

而且，抽签撕下了面纱，让悲剧无法避免赤祼祼地摆到社会面前。社会其实不能承受这种真相。

所以，社会也用惯例、习俗、文化来掩盖对悲剧的分配，比如印度留存至今的种姓制度。可是，这种表面上无分配的分配有其代价：社会假装习以为常，披上虚伪面纱。

总而言之，没有一个单一的办法能够长期地解决问题，抽签、习俗、政府、市场。社会要保有道德自信，就得将悲剧的分配伪装起来。卡拉布雷西认为， 单一策略的效果往往不如混合策略，就是既不是全靠市场也不能全靠政府，或者是抽签或惯例，而是多种分配方式的杂揉。

即便如此，稀缺永恒，每个社会都在做六个杯子五个盖的腾挪，能玩就玩下去，玩不下去了，社会就只好重置游戏。

也就是说， 社会的道德算法必然会周期性切换：因为无法在冲突的价值观中作出取舍，所以社会都保有它们，一个都不能少，但在时间上分开。 

某个时段某个价值观占上风，直到它承载的负能量过多，终被另一个价值观所取代。风水轮流转。这是社会道德算法的跨时多元化策略，为相互冲突但都被珍视的基本价值观留下火种，并缓解冲击。

回到自动汽车的话题，自动汽车时代注定很快到来，哪怕不会有一个公认「正确」的道德算法。未来自动汽车里植入的算法是厂商自决、市场选择还是政府规定，都有可能。

惟一确定的就是不管用哪个算法，都必然将制造属于它特有的那一类悲剧，等到这些悲剧沉重到社会必须切换另一种悲剧来承受时，齿轮转动，算法重置，悲剧的分配重新开始。

## 本讲小结

今天我们讨论的，是人工智能时代下的新电车难题。

无论是怎样的道德算法，本质上讨论的，都是面对社会困境把悲剧分配给谁。理论上不存在满足各种公平正义要求的正解，但实践中我们随时都在分配。

推荐给你一本书，卡拉布雷西的名著《悲剧性选择》。

## 今日思考

今天留一个互动： 如果你是乘客，你愿意让人工智能汽车在面对可能的车祸时，替你作出决定吗？你希望他用哪些道德算法来做决定？ 
欢迎你到文稿下方留言区留言。也欢迎你把今天的内容分享给你的朋友，邀请他一起来讨论。

明天，我们进入30天认知训练的最后一个模块。我会跟你讨论一个有趣的问题，关于女性，男人什么都不懂。


